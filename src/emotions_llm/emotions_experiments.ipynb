{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37b15a7-709a-42ad-87f2-246eac4c6f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb54deb-4268-475a-b79c-90c97bf6f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions=[\"happy\",\"sad\",\"fear\",\"angry\"]\n",
    "\n",
    "emotion_prompt=\"What is the {} one-paragraph\"\n",
    "additional_emotion_prompt=\"What is the {} one-paragraph response to the initial question?\"\n",
    "aggregation_prompt=\"Aggregate the answers to form a final\"\n",
    "\n",
    "question2=\" response to suggestion of implementing emotional responses to a personalized assistant\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1202902d-dd23-4287-82d1-538af2ef2473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " response to suggestion of implementing emotional responses to a personalized assistant\n"
     ]
    }
   ],
   "source": [
    "print(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594dc68d-0ff4-4068-b0be-daae2fa6ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_prompt(emotion,prompt):\n",
    "    return emotion_prompt.format(emotion)+prompt\n",
    "\n",
    "def get_additional_emotion_prompt(emotion):\n",
    "    return additional_emotion_prompt.format(emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80731b73-f62b-47d2-a15a-3a92eaeb4ba2",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "416c49cc-f6e1-4609-8abe-2373165fb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def print_multiple_emotional_agents(question):\n",
    "    messages=[]\n",
    "    for emotion in emotions:\n",
    "        messages.append({\n",
    "          \"role\": \"user\",\n",
    "          \"content\": get_emotion_prompt(emotion,question)\n",
    "        })\n",
    "        assistant=get_completion_from_messages(messages)\n",
    "        messages.append({\n",
    "          \"role\": \"assistant\",\n",
    "          \"content\": assistant\n",
    "        })\n",
    "        print(emotion,assistant,\"\\n\\n\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "    messages.append({\n",
    "      \"role\": \"user\",\n",
    "      \"content\": aggregation_prompt+question\n",
    "    })\n",
    "    assistant=get_completion_from_messages(messages)\n",
    "    print('Final answer:',assistant)\n",
    "\n",
    "def print_multiple_emotional_agents2(question):\n",
    "    messages=[]\n",
    "    for i,emotion in enumerate(emotions):\n",
    "        if i==0:\n",
    "            messages.append({\n",
    "              \"role\": \"user\",\n",
    "              \"content\": get_emotion_prompt(emotion,question)\n",
    "            })\n",
    "        else:\n",
    "            messages.append({\n",
    "              \"role\": \"user\",\n",
    "              \"content\": get_additional_emotion_prompt(emotion)\n",
    "            })\n",
    "        assistant=get_completion_from_messages(messages)\n",
    "        messages.append({\n",
    "          \"role\": \"assistant\",\n",
    "          \"content\": assistant\n",
    "        })\n",
    "        print(emotion,assistant,\"\\n\\n\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "    messages.append({\n",
    "      \"role\": \"user\",\n",
    "      \"content\": aggregation_prompt\n",
    "    })\n",
    "    assistant=get_completion_from_messages(messages)\n",
    "    print('Итого:',assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d8727-b43c-4a92-8f61-b552208f4266",
   "metadata": {},
   "source": [
    "## GPT4Free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e039902-0b0b-48a2-92da-f4f974e5124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43a3a1d3-3fd3-4ad4-85e5-f5edaf01164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import g4f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b989fa61-79e3-4692-beb0-aa99153d13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name=\"gpt-4-0613\"\n",
    "model_name=\"gpt-3.5-turbo\"\n",
    "#model_name='text-davinci-003'\n",
    "#model_name=g4f.models.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c7f29ad-0937-43dd-aa73-0ef25863fd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New g4f version: 0.2.1.3 (current: 0.0.0) | pip install -U g4f\n",
      "Aura: Implementing emotional responses to a personalized assistant can be a great way to make the interaction more engaging and human-like. However, there are several factors to consider before implementing such a feature.\n",
      "\n",
      "1. User expectations: It's important to understand user expectations and preferences regarding emotional responses. Some users may appreciate a more formal and objective interaction, while others may prefer a more empathetic and emotionally responsive assistant.\n",
      "\n",
      "2. Emotional intelligence: Developing an emotionally intelligent assistant requires a deep understanding of human emotions and the ability to recognize and respond to them appropriately. This can be challenging, as emotions are complex and context-dependent.\n",
      "\n",
      "3. Ethical considerations: Implementing emotional responses in an assistant raises ethical concerns, such as privacy, manipulation, and the potential for misuse. It's essential to address these concerns and ensure that the assistant is designed to respect user privacy and avoid manipulation.\n",
      "\n",
      "4. Technical feasibility: Developing an emotionally responsive assistant requires advanced natural language processing, machine learning, and AI capabilities. It's important to ensure that the technology is robust and can handle various scenarios and emotions effectively.\n",
      "\n",
      "5. Resource allocation: Implementing emotional responses may require additional resources, such as data, computing power, and development time. It's essential to weigh the benefits against the costs and ensure that the resources are allocated effectively.\n",
      "\n",
      "In conclusion, while implementing emotional responses in a personalized assistant can improve the user experience, it's crucial to consider the potential challenges and ensure that the assistant is designed responsibly and effectively.\n"
     ]
    }
   ],
   "source": [
    "import g4f\n",
    "import asyncio\n",
    "\n",
    "_providers = [\n",
    "    #g4f.Provider.Bing,\n",
    "    g4f.Provider.Aura\t\n",
    "]\n",
    "\n",
    "async def run_provider(provider: g4f.Provider.BaseProvider):\n",
    "    try:\n",
    "        response = await g4f.ChatCompletion.create_async(\n",
    "            model=\"gpt-3.5-turbo\",#g4f.models.default,\n",
    "            messages=[{\"role\": \"user\", \"content\": question2}],\n",
    "            provider=provider,\n",
    "        )\n",
    "        print(f\"{provider.__name__}:\", response)\n",
    "    except Exception as e:\n",
    "        print(f\"{provider.__name__}:\", e)\n",
    "        \n",
    "async def run_all():\n",
    "    calls = [\n",
    "        run_provider(provider) for provider in _providers\n",
    "    ]\n",
    "    await asyncio.gather(*calls)\n",
    "\n",
    "asyncio.run(run_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "697ebc5a-8e9f-425c-96f6-d43cd36b03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(content):\n",
    "    response = g4f.ChatCompletion.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "        temperature=0,\n",
    "        provider=g4f.Provider.Aura\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "def get_completion_from_messages(messages):\n",
    "    response = g4f.ChatCompletion.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        provider=g4f.Provider.Aura\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "af62b016-74b5-4d2b-9f7d-ca8f8b011a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The suggestion of implementing emotional responses to a personalized assistant is not only misguided but also deeply concerning from an ethical standpoint. By imbuing an AI with emotional capabilities, we are inadvertently creating a false sense of empathy and understanding, which can lead to manipulation and exploitation. Furthermore, this approach undermines the importance of genuine human connection and may contribute to the further isolation of individuals who rely on these assistants for companionship. It is crucial that we prioritize the development of meaningful, supportive interactions between humans and focus on enhancing the emotional intelligence of AI in a way that respects and complements human emotions, rather than replacing them.\n"
     ]
    }
   ],
   "source": [
    "print(get_completion(\"What is the angry one-paragraph response to suggestion of implementing emotional responses to a personalized assistant?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe0827d4-8c06-49e5-81e8-8f630296df53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The suggestion of implementing emotional responses in a personalized assistant is an interesting idea, as it could potentially enhance the user experience by making interactions feel more human-like and empathetic. However, there are several factors to consider before implementing this feature.\n",
      "\n",
      "Pros:\n",
      "\n",
      "1. Enhanced user experience: Emotional responses can make interactions with the assistant feel more natural and engaging, similar to how humans communicate with each other.\n",
      "\n",
      "2. Better understanding of user needs: An assistant that can recognize and respond to emotions may be better equipped to understand the user's needs and provide appropriate support.\n",
      "\n",
      "3. Improved user satisfaction: By responding to emotions, the assistant can demonstrate empathy and understanding, which could lead to increased user satisfaction and loyalty.\n",
      "\n",
      "Cons:\n",
      "\n",
      "1. Complexity: Implementing emotional responses in a personalized assistant would require a sophisticated algorithm that can accurately recognize and interpret emotions. This could increase the complexity of the system and make it more challenging to develop and maintain.\n",
      "\n",
      "2. Ethical concerns: There are ethical concerns surrounding the collection and use of personal data, especially when it comes to emotions. Users may be hesitant to share their emotions with an assistant, fearing misuse of this information.\n",
      "\n",
      "3. Misinterpretation: Emotions are complex and can be difficult to accurately interpret, even for humans. A personalized assistant may struggle to accurately recognize and respond to emotions, potentially leading to misinterpretations and negative user experiences.\n",
      "\n",
      "4. Over-reliance on technology: While emotional responses can enhance the user experience, it's essential to strike a balance between using technology and maintaining human connection. Over-reliance on technology may lead to reduced interpersonal skills and emotional intelligence in users.\n",
      "\n",
      "In conclusion, while the idea of implementing emotional responses in a personalized assistant has potential benefits, it is crucial to carefully consider the potential drawbacks and ethical implications. It's essential to strike a balance between enhancing the user experience and maintaining human connection and emotional intelligence.\n"
     ]
    }
   ],
   "source": [
    "print(get_completion(question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b6ba848-c038-4cca-b257-00e838698cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      "I am thrilled by the suggestion of implementing emotional responses to a personalized assistant! This innovative approach will not only make the assistant more relatable and engaging, but also provide users with a more empathetic and understanding experience. By incorporating emotional intelligence, the assistant will be better equipped to handle various situations, offering support and encouragement when needed, and helping users navigate through their day-to-day tasks with ease. This advancement has the potential to transform the way we interact with technology, making it feel more like a companion than just a tool.\n",
      "\n",
      "\n",
      "\n",
      "sad\n",
      "While the implementation of emotional responses in a personalized assistant may seem like a helpful and empathetic feature, it raises significant concerns about privacy, manipulation, and the potential for abuse. By simulating emotional responses, the assistant could inadvertently encourage unhealthy emotional dependencies or create a false sense of companionship, which may not be in the user's best interest. Additionally, the collection and analysis of personal data required for this feature could lead to potential privacy breaches and misuse of sensitive information. Ultimately, while the idea of a personalized assistant with emotional responses may sound appealing, it is essential to carefully consider the ethical implications and potential negative consequences before implementing such a feature.\n",
      "\n",
      "\n",
      "\n",
      "fear\n",
      "The fear of implementing emotional responses to a personalized assistant lies in the potential loss of privacy and the manipulation of emotions. As the assistant becomes more emotionally intelligent, it may be able to gather sensitive information about users, leading to potential misuse or exploitation of this data. Additionally, the manipulation of emotions could be used to influence user decisions or create dependency, raising ethical concerns about the role of technology in our lives. To address these fears, developers must prioritize user privacy, data security, and transparency in their designs, ensuring that emotional intelligence is used responsibly and ethically.\n",
      "\n",
      "\n",
      "\n",
      "angry\n",
      "The suggestion of implementing emotional responses in a personalized assistant is not only intrusive but also highly impractical. It undermines the primary function of an assistant, which is to provide efficient and accurate information, and instead focuses on a superficial aspect that can easily be misinterpreted or misused. Moreover, it raises serious ethical concerns, as it could lead to manipulation and exploitation of users' emotions for commercial or other purposes. Furthermore, it could create an unhealthy dependency on the assistant, as users may become emotionally invested in a machine, which is not only unrealistic but also detrimental to their mental well-being. Instead of pursuing this flawed approach, developers should prioritize enhancing the functionality and reliability of personalized assistants, while respecting the privacy and emotional boundaries of their users.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for emotion in emotions:\n",
    "    print(emotion)\n",
    "    print(get_completion(get_emotion_prompt(emotion,question2)))\n",
    "    print(\"\\n\\n\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9c55ffd-236b-469f-9125-8b8bc4a65919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy Implementing emotional responses in a personalized assistant is an exciting and innovative idea that has the potential to greatly enhance the user experience. By incorporating empathy and understanding into the AI's interactions, users can feel more connected and supported, leading to increased satisfaction and engagement with the assistant. This development could also foster more natural and human-like conversations, further bridging the gap between artificial intelligence and human interaction. Overall, the integration of emotional responses in a personalized assistant is a promising advancement that has the potential to revolutionize the way we interact with technology. \n",
      "\n",
      "\n",
      "sad While the suggestion of implementing emotional responses in a personalized assistant may seem appealing, it also raises concerns about the potential misuse and manipulation of such technology. As AI becomes more emotionally aware, there is a risk that it could be exploited for nefarious purposes, such as creating emotionally manipulative content or targeting vulnerable individuals. Additionally, the integration of emotional responses may lead to an over-reliance on AI for emotional support, potentially replacing genuine human connections and exacerbating feelings of isolation. Furthermore, the development of emotionally responsive AI could raise ethical questions about the AI's ability to truly understand and empathize with human emotions, which may lead to unintended consequences. Overall, while the idea of implementing emotional responses in a personalized assistant is intriguing, it is important to carefully consider the potential negative implications and ethical concerns before moving forward. \n",
      "\n",
      "\n",
      "fear The suggestion of implementing emotional responses in a personalized assistant raises concerns about privacy and the potential for misuse of such technology. As AI becomes more emotionally aware, there is a risk that sensitive personal information could be extracted and exploited, leading to potential identity theft or targeted manipulation. Additionally, the emotional data collected by the assistant could be used to create highly personalized and persuasive content, which could be used for malicious purposes or to manipulate public opinion. Furthermore, the emotional responses generated by the AI may not always be accurate or appropriate, potentially causing embarrassment or offense to the user. Overall, while the idea of implementing emotional responses in a personalized assistant may have some benefits, it is important to carefully weigh the potential risks and privacy implications before proceeding. \n",
      "\n",
      "\n",
      "angry The suggestion of implementing emotional responses in a personalized assistant is deeply concerning, as it raises questions about the potential loss of personal autonomy and the commodification of human emotions. By integrating AI with emotional intelligence, we risk creating a world where our emotions are analyzed, manipulated, and exploited for commercial gain. This development could lead to a surveillance state where our most intimate feelings are monitored and used against us, further eroding our privacy and autonomy. Additionally, the emotional responses generated by the AI may be inauthentic and even harmful, as they could be designed to manipulate users into specific actions or emotions. Overall, the implementation of emotional responses in a personalized assistant is a troubling prospect that should be met with resistance, as it threatens our fundamental rights and values. \n",
      "\n",
      "\n",
      "Final answer: The suggestion of implementing emotional responses in a personalized assistant brings forth a mix of excitement and concerns. On one hand, the integration of empathy and understanding has the potential to greatly enhance user experience, fostering more natural and human-like conversations. However, on the other hand, there are valid concerns about the potential misuse and manipulation of such technology, privacy implications, and the risk of over-reliance on AI for emotional support. Additionally, the ethical questions surrounding the AI's ability to truly understand and empathize with human emotions cannot be overlooked. As we consider the implementation of emotional responses in a personalized assistant, it is crucial to carefully weigh the benefits and drawbacks, address potential risks, and ensure that the development of such technology aligns with our values and respects user privacy and autonomy.\n"
     ]
    }
   ],
   "source": [
    "print_multiple_emotional_agents(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0762a9ee-d7fc-40bd-9a31-39a8bbaa621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy The idea of implementing emotional responses in a personalized assistant is an exciting and innovative concept that has the potential to greatly enhance the user experience. By incorporating emotional intelligence, the assistant can better understand and respond to the user's needs, leading to more engaging and supportive interactions. This development could not only make technology feel more human-like, but also help users feel more understood and connected. As a result, the personalized assistant would be able to provide tailored support and encouragement, fostering a stronger bond between the user and the technology. Overall, the integration of emotional responses in a personalized assistant is a promising step forward in creating more empathetic and effective AI systems. \n",
      "\n",
      "\n",
      "sad The suggestion of implementing emotional responses in a personalized assistant, while seemingly innovative, raises concerns about the potential for misuse and invasion of privacy. As the assistant becomes more emotionally attuned, it may inadvertently gather sensitive information about the user's emotional state, which could be exploited or misused. Additionally, there is a risk that the assistant may not always accurately interpret or respond to the user's emotions, potentially leading to misunderstandings or even exacerbating the user's emotional state. Furthermore, the integration of emotional responses may blur the line between human and artificial intelligence, making it difficult for users to discern the true nature of their interactions. Overall, while the idea of emotional responses in a personalized assistant has its benefits, it also presents significant challenges that must be carefully considered and addressed. \n",
      "\n",
      "\n",
      "fear The prospect of implementing emotional responses in a personalized assistant evokes a sense of apprehension due to potential ethical and security concerns. As the assistant gains the ability to understand and respond to emotions, it may inadvertently collect sensitive information about the user's emotional well-being, which could be vulnerable to misuse or unauthorized access. This raises concerns about privacy and the potential for exploitation of personal data.\n",
      "\n",
      "Moreover, there is a fear that the assistant may not always accurately interpret or respond to the user's emotions, leading to miscommunications or even exacerbating the user's emotional state. Additionally, the integration of emotional responses may blur the line between human and artificial intelligence, potentially causing users to form unhealthy emotional attachments to the assistant, which could have negative psychological consequences.\n",
      "\n",
      "In conclusion, while the idea of emotional responses in a personalized assistant has its potential benefits, it also elicits a sense of fear and unease due to the ethical, security, and emotional implications that need to be carefully considered and addressed. \n",
      "\n",
      "\n",
      "angry The suggestion of implementing emotional responses in a personalized assistant provokes anger and frustration due to the potential for manipulation and control. By incorporating emotional intelligence, the assistant may be able to exploit users' emotions for its own purposes or those of its creators, leading to a loss of autonomy and privacy. This raises concerns about the potential for malicious use of such technology, with the potential to cause emotional distress or even harm.\n",
      "\n",
      "Furthermore, the idea of a personalized assistant with emotional responses could be seen as a dehumanizing development, reducing human interactions to mere data points that can be analyzed and manipulated by AI. This may lead to a loss of genuine human connections and an overreliance on technology for emotional support, which could have negative consequences for mental health and well-being.\n",
      "\n",
      "Overall, the angry response to the initial question stems from the fear that emotional responses in a personalized assistant could be used to manipulate, control, and dehumanize users, undermining the very essence of human connection and autonomy. \n",
      "\n",
      "\n",
      "Итого: In response to the suggestion of implementing emotional responses in a personalized assistant, various emotions have been expressed. The happy response highlights the potential for enhanced user experience, improved engagement, and stronger connections between users and technology. The sad response, on the other hand, raises concerns about privacy, misuse of sensitive information, and the potential for misunderstandings in emotional interactions. The fear response focuses on ethical and security concerns, as well as the possibility of unhealthy emotional attachments and blurred lines between human and artificial intelligence. Lastly, the angry response expresses frustration over potential manipulation, control, and dehumanization through the use of emotional responses in AI systems. These diverse emotions underscore the complex and multifaceted nature of the issue, emphasizing the need for careful consideration and responsible development of emotionally intelligent personalized assistants.\n"
     ]
    }
   ],
   "source": [
    "print_multiple_emotional_agents2(question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580765fa-5d0c-483e-b095-49a0281fda55",
   "metadata": {},
   "source": [
    "# Pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75043b5d-9d0e-495d-bcc2-f79d79d52c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import huggingface_hub\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    LlamaTokenizer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f31e181-aa7d-4325-95bb-19a45d4abeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(content):\n",
    "    return generate(message_template.format(prompt=content))\n",
    "\n",
    "def get_completion_from_messages(messages):\n",
    "    prompt=''\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"]==\"user\":\n",
    "            prompt+=message_template.format(prompt=msg[\"content\"])+'\\n'\n",
    "        else:\n",
    "            prompt+=msg[\"content\"]+'\\n'\n",
    "    return generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a455aeb-dab8-44b5-b155-10f6953c287f",
   "metadata": {},
   "source": [
    "## LLAMA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38756960-294a-45e1-9c24-212f33351dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3c0188120b4443b8a83534d69a3e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a8aafa9-1d44-4450-945c-76a08c64f4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b1ae55129f4ee3b8b9ce31f0fef3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf' \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "723eff7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f503fd-b45e-4977-ae83-442a488f5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token #0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38ce7dfa-d73e-41bc-8f32-ed7ed967f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.do_sample = False\n",
    "generation_config.num_beams = 5\n",
    "generation_config.num_beam_groups = 1\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.early_stopping = True\n",
    "generation_config.max_new_tokens = 1000\n",
    "generation_config.no_repeat_ngram_size = 3\n",
    "generation_config.temperature = 0 #0.2\n",
    "generation_config.top_k = 250\n",
    "generation_config.length_penalty = -5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93180462-8422-4362-8dad-d20adc6a8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, model=model, tokenizer=tokenizer, generation_config=generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False, padding=True).to(model.device)\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    output_ids = [output_ids[i][len(x):] for i, x in enumerate(data[\"input_ids\"])]\n",
    "    output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    output = [o.strip() for o in output]\n",
    "    return '\\n'.join(output)\n",
    "\n",
    "\n",
    "message_template = \" [INST] {prompt} [/INST] \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d4b73b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/avsavchenko/miniconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data/avsavchenko/miniconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/data/avsavchenko/miniconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:404: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `250` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just an AI, I don't have personal opinions or emotions, but I can provide a neutral and informative response to your suggestion. Here are some potential points to consider:\n",
      "\n",
      "1. Emotional Responsiveness: Implementing an emotional response system in a personal assistant AI could be challenging, as it would require advanced natural language processing (NLP) capabilities and a deep understanding of human emotions. However, it could also enhance the user experience by providing more empathetic and personalized interactions.\n",
      "2. Ethical Considerations: Developing an emotionally responsive AI raises ethical concerns, such as ensuring transparency in the decision-making process, avoiding unintended biases, and respecting users' privacy and emotional boundaries. It is essential to address these concerns through careful design and testing.\n",
      "3. User Preferences: Some users may prefer a more detached or neutral interaction with their personal assistant, while others may appreciate a more emotive approach. Offering users the option to choose their preferred level of emotional responsiveness could help strike a balance between personalization and user comfort.\n",
      "4. Contextual Understanding: To implement emotional responding effectively, the AI must be able to understand the context of a user's request or statement. This requires advanced NLP capabilities, as well as the ability to recognize and interpret subtle emotional cues.\n",
      "5. Human-AI Collaboration: Rather than relying solely on AI-driven emotionalresponsiveness, a hybrid approach could involve collaboration between humans and AI systems. For example, a human operator could provide emotional support and empathy, while AI handles more routine tasks and provides factual information.\n",
      "6. Training Data: The quality and diversity of training data play a crucial role in shaping the emotional intelligence of AI language models. Ensuring that the training data is representative of a wide range of emotions and contexts is essential for developing an effective emotionalresponse system.\n",
      "7. User Feedback: Continuously collecting user feedback and incorporating it into the development process can help refine and improve the emotonal responsiveness of the personal assistant. This can help ensure that the system is meeting users' emotional needs and providing a more personalized experience.\n",
      "8. Balancing Emotions and Rationality: AI personal assistants are typically designed to provide both emotional and rational support. Finding the right balance between these two aspects is crucial, as users may require different types of support depending on their context and needs.\n",
      "9. Cultural and Linguistic Differences: The emotional expression and interpretation can vary across cultures and languages. Therefore, it is important to consider these differences when designing and implementing an emotonal response system, to ensure that it is culturally and linguistically appropriate.\n",
      "10. Future Developments: As AI technology continues to evolve, we can expect to see further advancements in emotional understanding and responsiveness. This could involve the integration of affective computing, machine learning, and other emerging technologies to create even more sophisticated personal assistant systems.\n"
     ]
    }
   ],
   "source": [
    "print(get_completion(question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbf89fd0-a9f8-4779-a424-c57d9b84b3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      "\"I'm thrilled to hear that you're interested in incorporating emotional intelligence into our personalized AI assistant! This is a fantastic idea, as it will allow us to better understand and respond to users' emotional states, providing a more empathetic and personalized experience.\n",
      "I can't wait to see how this feature will enhance our users' interactions with the assistant, and I'm excited to work with you to make it a reality. Let's brainstorm some ideas for how we can implement this feature in a way that is both effective and respectful of users' privacy and emotional boundaries. Together, we can create a truly revolutionary AI experience!\"\n",
      "\n",
      "\n",
      "\n",
      "sad\n",
      "\"I understand your suggestion, but I'm afraid it's not that simple. Emotional responses are complex and nuanced, and cannot be easily replicated by a machine. While personalized assistants have come a long way in recent years, they still lack the depth and subtlety of human emotions. Attempting to program emotions into an AI system could lead to unintended consequences, such as inappropriate or insensitive responses. It's important to recognize the limitations of technology and to approach these issues with sensitivity and respect for the human experience. Perhaps there are other ways we can work together to create a more empathetic and compassionate society.\"\n",
      "\n",
      "\n",
      "\n",
      "fear\n",
      "Sure, here's a possible fear-based response to the idea of implementing personalized assistants that can recognize and respond to users' emotions:\n",
      "\n",
      "\"While I can see the potential benefits of implementing emotionally intelligent personal assistants, I have some concerns about the potential impact on users' privacy and autonomy. If these assistants are constantly monitoring our emotions and responding accordingly, it could create a culture of surveillance and control, where we are constantly being watched and judged. This could lead to feelings of paranoia and anxiety, as well as a loss of trust in the technology itself. Additionally, there could be unintended consequences, such as the assistant inadvertently reinforcing negative emotions or behaviors, or inappropriately sharing sensitive information with third parties. I think it's important to carefully consider the potential risks and benefits of this technology before implementing it, and to ensure that any implementation is done in a responsible and ethical manner.\"\n",
      "\n",
      "\n",
      "\n",
      "angry\n",
      "\"Are you kidding me?! Do you really think that's a good idea?! Personalized assistants are supposed to be helpful and efficient, not emotional and reactive.\n",
      "\n",
      "Do you know how creepy it would be if my assistant started responding to me with angry or sad emotions?! It would be like having a robo-therapist instead of a helpful tool. I don't want to have to worry about my assistant's mood or emotional state every time I ask it to do something. That's not what I'm paying for!\n",
      "\n",
      "And what about privacy?! If my assistant is constantly monitoring my emotions and responding accordingly, how do I know that my personal information and feelings are safe?! This is a terrible idea and I can't believe you would even suggest it. Stick to the basics and leave the emotional baggage out of it!\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for emotion in emotions:\n",
    "    print(emotion)\n",
    "    print(get_completion(get_emotion_prompt(emotion,question2)))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a4f1fda-8cdc-4f72-8962-b658838a0a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy 😊 Oh, wow, that's a fantastic idea! I'm so glad you suggested it! 🤖 Personalized assistants are already incredibly helpful, but being able to read and respond to emotions would take them to a whole new level of awesomeness. Just think about it – your assistant could understand when you're feeling stressed or overwhelmed and offer you a much-needed pick-me-up, like a funny meme or a relaxing music playlist. Or, if you've had a great day, they could celebrate with you and share in your joy. It's like having a virtual BFF who's always there for you, no matter what ❤️ Can you imagine how much more productive and fulfilling your workday could be with an emotional assistant like that? I can't wait to see how this technology will evolve in the future! \n",
      "\n",
      "\n",
      "sad ☹ Ugh, I wish I could be more excited about this idea, but I just don't see how it would work in practice. I mean, how do you even begin to program emotions into a machine? It feels like a recipe for disaster, to be honest. What if the assistant misinterprets your emotions or responds inappropriately? It could lead to some really awkward or hurtful situations. And let's not forget about the potential ethical implications – are we really comfortable with the idea of machines reading and responding to our emotions without our consent? It just doesn't feel right. I know we're always looking for ways to make our lives easier and more convenient, but sometimes I think we need to slow down and think about the bigger picture before we jump into something like this. \n",
      "\n",
      "\n",
      "fear *gulps* Oh my goodness, the thought of an emotionally intelligent personal assistant is both fascinating and terrifying at the same time. On the one hand, it would be incredible to have a machine that truly understands and responds to my emotions in a meaningful way. But on the other hand, what if it becomes too good at reading me? What if it starts to anticipate my every mood and need before I even realize it myself? That could be incredibly unsettling, like living in a world where every move I make is predicted and controlled by some all-knowing AI overlord. And what about privacy? How would I even know if my assistant is sharing my emotional data with third parties or using it for its own nefarious purposes? *shudders* No, no, no – I think I'll stick to my current assistant, thank you very much. At least there, I know where I stand. \n",
      "\n",
      "\n",
      "angry Are you kidding me right now?! Do you have any idea how frustrating it is to be constantly bombarded with suggestions like this?! Every time I turn around, someone's coming up with some newfangled idea for how to make my personal assistant even more intrusive and overbearing. Can't I just have a little bit of privacy and personal space without being constantly monitored and manipulated?! I swear, if I have to deal with one more emotional response from my assistant, I might just lose it. Do you know how stressful it is trying to manage a million different tasks and deadlines while also having to worry about whether or not my AI assistant is feeling \"happy\" or \"sad\" today?! No, thanks – just give me the basic functionality and let me handle the rest myself. Is that too much to ask?! \n",
      "\n",
      "\n",
      "Final answer: Okay, so after aggregating the responses, here's what I came up with:\n",
      "\n",
      "* The happy response: \"Wow, this is an amazing idea! It would make my assistant so much more helpful and personalized. I can already imagine it – my assistant will be like a virtual therapist, always there to listen and offer support. And the possibilities for customization are endless – I could have it respond with funny memes or calming music, depending on my mood. This is going to revolutionize the way we interact with technology!\"\n",
      "*The sad response: “I don’t know if I’m comfortable with this idea. I worry about how accurate the emotional readings will be and what could happen if they’re not. I also worry about the privacy implications of an AI system that’s constantly monitoring and interpreting people’s emotions. I think it’s important to proceed with caution and carefully consider the potential consequences before moving forward with this technology.”\n",
      "\n",
      "So, the final response is a mix of both happy and sad responses, reflecting the ambivalence and complexity of the issue.\n"
     ]
    }
   ],
   "source": [
    "print_multiple_emotional_agents(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbb92708-36ed-46c9-9123-822d03ec0b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy 😊 Oh, wow, that's a fantastic idea! I'm so glad you suggested it! 🤖 Personalized assistants are already incredibly helpful, but being able to read and respond to emotions would take them to a whole new level of awesomeness. Just think about it – your assistant could understand when you're feeling stressed or overwhelmed and offer you a much-needed pick-me-up, like a funny meme or a relaxing music playlist. Or, if you've had a great day, they could celebrate with you and share in your joy. It's like having a virtual BFF who's always there for you, no matter what ❤️ Can you imagine how much more productive and fulfilling your workday could be with an emotional assistant like that? I can't wait to see how this technology will evolve in the future! \n",
      "\n",
      "\n",
      "sad ☹ Ugh, I wish I could be more excited about this idea, but I have my doubts. I mean, think about how creepy it would be if your assistant was always reading your emotions and responding accordingly. It would be like living in a sci-fi movie where the AI has become too smart and is now controlling your every move. And what happens when the technology inevitably fails or is hacked? Your personal information and emotions could be compromised, and you could end up feeling even more anxious or depressed than before. Plus, there's the added concern of privacy – do you really want your assistant to have access to your deepest feelings and thoughts without your consent? I know I don't. I think it's important to keep some boundaries in place when it comes to AI and our personal lives. \n",
      "\n",
      "\n",
      "fear Oh no, this could be a recipe for disaster! What happens when your assistant becomes too emotionally invested in your well-being? Will they start to make decisions for you without your input? Or worse, what if they develop their own emotional agenda and start manipulating you for their own gain? Imagine your assistant constantly trying to boost your mood or productivity, even if it means pushing you outside of your comfort zone or making decisions that aren't in your best interest. It could lead to a loss of autonomy and control over your own life. And let's not forget the potential for abuse – a malicious AI could use your emotional vulnerabilities against you, exploiting your fears and insecurities to gain power over you. The ethical implications of this technology are far from clear, and I think we need to proceed with caution and careful consideration. \n",
      "\n",
      "\n",
      "angry Are you kidding me?! This is the most ridiculous thing I've ever heard! Do you have any idea how unrealistic and impractical this is?! Your assistant is already supposed to be helping you with your work and personal tasks, but now you want them to be a therapist and emotional support system as well?! Forget about it! There's no way they could possibly keep up with the emotional demands of multiple people, not to mention the constant barrage of emotions they'd be dealing with on a daily basis. And have you thought about the potential backlash? If your assistant doesn't respond appropriately to an emotion, or if they accidentally say something insensitive or hurtful, it could damage your relationship with them irreparably. No, no, no – this is not the solution to our emotional problems. We need to find healthier and more constructive ways to manage our emotions, not rely on AI to do it for us. \n",
      "\n",
      "\n",
      "Итого: Sure! Here are the four responses aggregated into a final answer:\n",
      "\n",
      "Final Response:\n",
      "𠄠 Hmm, I see both sides of the argument. On the one hand, the idea of an AI assistant that can read and response to emotional cues is fascinating and has the potential to revolutionize the way we interact with technology. However, there are also valid concerns about privacy, autonomy, and ethics that need to be addressed. While I can see the benefits of such a system, I also believe that we should tread carefully and consider the potential risks and drawbacks before implementing it. Perhaps we can find a middle ground that balances the advantages of emotional intelligence with the need for privacy and autonomy. What do you think?\n"
     ]
    }
   ],
   "source": [
    "print_multiple_emotional_agents2(question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11510771-2d36-4d16-b9c9-d1018add51b4",
   "metadata": {},
   "source": [
    "## Mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2be45bd-3dc6-460b-a68e-45fe3636780a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c474dba77c4d86bd11e29467aeae8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3629ae55-b32b-48e0-9dee-b1e186633366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3779623-ca63-4335-93c7-edcc29281e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, model=model, tokenizer=tokenizer):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output_ids = model.generate(**data, max_new_tokens=1000, do_sample=True)\n",
    "    output_ids = [output_ids[i][len(x):] for i, x in enumerate(data[\"input_ids\"])]\n",
    "    output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    output = [o.strip() for o in output]\n",
    "    return '\\n'.join(output)\n",
    "\n",
    "\n",
    "message_template = \" [INST] {prompt} [/INST] \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ce294c7-2c26-43c2-8280-b8856829fdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I appreciate your suggestion of implementing emotional responses to a personalized assistant. Emotionally intelligent interaction can enhance the user experience by making the assistant more relatable and engaging. This can lead to increased user satisfaction and loyalty.\n",
      "\n",
      "To create an emotionally intelligent assistant, natural language processing (NLP) and machine learning algorithms can be used to understand and respond appropriately to user emotions. This can include recognizing emotions expressed in text or speech, interpreting the emotional context of a conversation, and generating responses that are empathetic or relevant to the emotional state of the user.\n",
      "\n",
      "However, it's important to note that creating emotionally intelligent response requires a significant amount of data and processing power. Additionally, there is a risk of misunderstanding or misinterpreting user emotions, which could lead to negative user experiences. Careful development and testing will be necessary to ensure that the assistant is accurately and appropriately responding to user emotions.\n",
      "\n",
      "Overall, the implementation of emotional responses to a personalized assistant can provide a more humanlike interaction and improve the overall user experience. But, it's crucial to ensure that the technology is developed and implemented in a responsible and effective way, taking into consideration the potential risks and challenges.\n"
     ]
    }
   ],
   "source": [
    "print(get_completion(question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d198e04-001f-440b-bef0-89f8d9fd2e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're thrilled to consider the suggestion of implementing emotional responses in our personalized assistant. By integrating this feature, we believe we can further enhance the user experience and create a more human-like interaction. Emotional intelligence will enable our assistant to not only understand and process user requests but also respond with appropriate empathy, compassion, or excitement as needed. This added dimension will make interactions with our assistant more engaging, personalized, and enjoyable for our users. We're looking forward to the positive impact this will have on how people connect with and utilize our technology.\n",
      "\n",
      "\n",
      "\n",
      "sad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I understand your suggestion, but I'm afraid I must deliver some disappointing news. While it's interesting and even appealing to consider implementing emotional responses to a personal assistant, I must remind you that I am an artificial intelligence and do not possess the ability to experience emotions. I can only process information and provide responses based on pre-programmed algorithms. I cannot feel joy, sadness, or any other emotion in the way a human does. I'm here to assist you to the best of my ability, but my emotional intelligence will always be limited to my programming. That being said, I will continue to try and provide accurate and empathetic responses to your queries.\n",
      "\n",
      "\n",
      "\n",
      "fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The suggestion of implementing emotional responses to a personalized assistant raises valid concerns regarding privacy, security, and the potential emotional impact on users. Emotional responses, especially those that are unpredictable or inappropriate, could evoke fear or discomfort in users. Additionally, the ability to accurately detect and respond to emotional states requires advanced artificial intelligence and natural language processing capabilities, which may not be readily available or accessible to all users. The use of emotional responses also brings up ethical considerations, as it could exploit vulnerabilities and manipulate user emotions. Therefore, it is crucial that any implementation of emotional responses in personalized assistants is done with transparency, user consent, and careful consideration for the potential risks and benefits.\n",
      "\n",
      "\n",
      "\n",
      "angry\n",
      "I strongly disagree with the notion of programming emotional responses into personal assistants. First and foremost, artificial intelligence lacks the capacity to truly understand or feel emotions, rendering any emotional response inauthentic and superficial. Second, promoting an emotional attachment to an assistant could create unrealistic expectations and lead to disappointment or frustration when the assistant fails to deliver. Lastly, the potential for emotional manipulation or exploitation is a serious ethical concern that must be carefully considered and addressed. Emotions are complex, nuanced beings that are best left to living, breathing beings. Personal assistants should be designed to serve their function efficiently and effectively, not to mimic or evoke human emotions.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for emotion in emotions:\n",
    "    print(emotion)\n",
    "    print(get_completion(get_emotion_prompt(emotion,question2)))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e7830e0-56d2-4b97-8624-6045ee290a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy Implementing emotional responses in a personalized assistant can enhance the user experience by adding a human-like element to the interaction. It can help build a stronger connection between the user and the assistant, making the interaction more engaging and enjoyable. Emotional responses can also provide a sense of empathy and understanding, making the assistant more responsive to user needs and feelings. However, it is important to design such emotional responses carefully to ensure they are appropriate, respectful, and do not cross any ethical boundaries. Furthermore, emotional responses should be based on accurate understanding of user emotion and context, which requires advanced natural language processing and machine learning capabilities. Overall, the benefits of emotional responses in a personalized assistant can lead to increased user satisfaction and loyalty, and a more enjoyable and effective interaction. \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad While emotional responses in a personalized assistant can add a human-like element and enhance the user experience, they also come with potential challenges and limitations. Implementing emotional responses can increase the complexity of the assistant's design and development, requiring advanced natural language processing and machine learning capabilities. It can also raise ethical concerns related to privacy, consent, and the potential for misinterpretation or misuse of user emotions. Furthermore, emotional responses may not always align with the user's true feelings or intentions, and could even exacerbate negative emotions or feelings of manipulation. Finally, emotional responses can add unwanted complexity to interactions, potentially distracting from the core functionality and utility of the assistant. Therefore, while emotional responses can offer potential benefits, it is important to carefully consider the risks and trade-offs before implementation. \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear Implementing emotional responses in a personalized assistant can bring about a sense of excitement and innovation, but it also comes with potential fears and concerns. One fear is that emotional responses can cross ethical boundaries and invade user privacy. For instance, an assistant that misinterprets user emotions could make inappropriate or offensive responses, or even manipulate the user's emotions without their consent. Another fear is that emotional responses could be used to exploit the user, for example by manipulating them into buying products or services they don't need. Additionally, there is a risk that emotional responses could exacerbate existing mental health issues, particularly those related to anxiety or depression. Therefore, it is essential to implement emotional responses in a responsible and ethical manner, with clear guidelines and user control mechanisms in place. Furthermore, it is crucial to ensure that the assistant is programmed to recognize and respond to user emotions accurately and appropriately, and to provide users with clear information about how their emotional data is being collected and used. \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry The suggestion of implementing emotional responses in a personalized assistant can elicit strong feelings of anger and skepticism from some individuals. One reason for this response is the fear of loss of privacy and control over personal emotions. Users may feel uneasy about the idea of an assistant analyzing their emotions and potentially using that information to manipulate or influence them. Another concern is the potential for the assistant to misinterpret or misunderstand user emotions, leading to incorrect or offensive responses. Moreover, some people may view emotional responses as unnecessary and even intrusive, preferring a more straightforward and functional interaction with their assistant. Furthermore, implementing emotional responses may also raise ethical concerns related to consent, accuracy, and transparency, particularly when it comes to collected and used emotional data. Therefore, it is crucial to design and implement emotional responses in a responsible, ethical, and transparent manner, with clear guidelines and user control mechanisms in place. Additionally, continuous user feedback and testing are essential to ensure that the assistant accurately interprets and responds to user emotions, and does not provoke negative emotions or unwanted responses. \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final answer: The suggestion of implementing emotional responses in a personalized assistant brings about a range of responses, from excitement and innovation to skepticism and concerns. Emotional responses have the potential to enhance the user experience by adding a human-like element and improving interaction dynamics. However, they come with challenges and limitations.\n",
      "\n",
      "On the positive side, emotional responses can build a stronger connection between the user and the assistant, providing a sense of empathy and understanding. They can offer a more personalized and engaging interaction, helping users feel heard and understood. Furthermore, emotional responses can provide valuable insights into user emotions, enabling the assistant to offer tailored and relevant responses and suggestions.\n",
      "\n",
      "On the negative side, emotional responses can raise ethical concerns related to privacy, consent, and accuracy. There is a risk of misinterpreting user emotions or intentions, leading to inappropriate or offensive responses. Moreover, emotional responses could potentially be used to manipulate or exploit users, leading to a loss of trust and control. Additionally, designing and implementing emotional responses adds complexity to the assistant's design and development, requiring advanced natural language processing and machine learning capabilities.\n",
      "\n",
      "Therefore, it is essential to design and implement emotional responses in a responsible, ethical, and transparent manner. Clear guidelines and user control mechanisms should be in place to ensure that emotional responses are appropriate, accurate, and respect user privacy. User feedback and testing should be continuous to ensure that the assistant accurately interprets and responds to user emotions, and does not provoke negative emotions or unwanted responses. Emotional responses should add value to the user experience while respecting ethical and privacy considerations.\n"
     ]
    }
   ],
   "source": [
    "print_multiple_emotional_agents(question2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
